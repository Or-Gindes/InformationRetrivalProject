{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b3c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "cluster-d6cc  GCE       2                                       RUNNING  us-central1-f\n",
      "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "cluster-d6cc  GCE       2                                       RUNNING  us-central1-f\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7647c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q google-cloud-storage==1.43.0\n",
    "# !pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7473c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk==3.7\n",
    "# !python -m nltk.downloader wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508edd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "# from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from math import log, pow\n",
    "from google.cloud import storage\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b42bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as pyFunc\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "# from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d623c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = '201640042_project' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name.endswith('parquet'):\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f937eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "doc_anchor_pairs = parquetFile.select(\"anchor_text\", \"id\").rdd\n",
    "n_pages = parquetFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b0c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea31e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverted_index_gcp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a4035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "# # # Opening JSON file\n",
    "# import json\n",
    "# with open('/home/dataproc/queries_train.json') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "# train_tokens = [token.group() for token in RE_WORD.finditer(' '.join(data.keys()).lower())]\n",
    "# train_filtered_vocab = set([lemmatizer.lemmatize(tok) for tok in train_tokens if (tok not in all_stopwords)])\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def get_tokens(text, doc_id):\n",
    "    \"\"\"Support function - performs token filtering per doc \n",
    "      Parameters:\n",
    "    -----------\n",
    "    text: str\n",
    "      Text of one document\n",
    "    id: int\n",
    "      Document id\n",
    "    Returns:\n",
    "    --------\n",
    "    List of tuples\n",
    "      A list of (doc_id, filtered_token_list) pairs \n",
    "    \"\"\"\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token != '']\n",
    "    filtered_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if (tok not in all_stopwords)]\n",
    "    return [(doc_id, filtered_tokens)]\n",
    "\n",
    "\n",
    "def word_count(doc_id, tokens):\n",
    "    \"\"\" Count the frequency of each word in `text` (tf) that is not included in \n",
    "    `all_stopwords` and return entries that will go into our posting lists. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    id: int\n",
    "      Document id\n",
    "    tokens: str\n",
    "      list of tokens from document\n",
    "    Returns:\n",
    "    --------\n",
    "    List of tuples\n",
    "      A list of (token, (doc_id, tf)) pairs \n",
    "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "    \"\"\"\n",
    "    doc_word_count = Counter(tokens)\n",
    "    return [(tok, (doc_id, tf)) for tok, tf in doc_word_count.items()]\n",
    "\n",
    "def get_doc_len(doc_id, tokens):\n",
    "    \"\"\" Count document filtered length for storage in index as well as document vector length for RDD calculations\n",
    "  Parameters:\n",
    "  -----------\n",
    "    id: int\n",
    "      Document id\n",
    "    tokens: str\n",
    "      list of tokens from document\n",
    "    Returns:\n",
    "  --------\n",
    "    List of tuples\n",
    "      A list of (doc_id, doc_length) pairs\n",
    "  \"\"\"\n",
    "    doc_word_count = Counter(tokens)\n",
    "    doc_len = sum(doc_word_count.values())\n",
    "    return [(doc_id, doc_len)]\n",
    "    \n",
    "\n",
    "def get_tfidf(tf, df, doc_len):\n",
    "    if doc_len == 0:\n",
    "        return 0.0\n",
    "    tf_idf = tf/doc_len * log(n_pages / (df + 1), 2)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6d9ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get title dict for inverted index to quickly prepare results\n",
    "id2title = doc_title_pairs.map(lambda x: (x[1], x[0])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ee63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bodyIndex with uncommon_words filter\n",
      "\n",
      "running bodyIndex with uncommon_words filter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostingLocs created for full_lemmatized_body_index\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document frequancy created for full_lemmatized_body_index\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc data created for full_lemmatized_body_index\n",
      "\n",
      "full_lemmatized_body_index written\n",
      "\n",
      "Copying file://full_lemmatized_body_index.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][297.8 MiB/297.8 MiB]                                                \n",
      "Operation completed over 1 objects/297.8 MiB.                                    \n",
      "full_lemmatized_body_index uploaded to bucket\n",
      "\n",
      "297.75 MiB  2023-01-10T21:04:46Z  gs://201640042_project/full_lemmatized_body_index/full_lemmatized_body_index.pkl\n",
      "TOTAL: 1 objects, 312218439 bytes (297.75 MiB)\n"
     ]
    }
   ],
   "source": [
    "# rdd_dict = {\"full_body_index\": doc_text_pairs, \"full_title_index\": doc_title_pairs, \"full_anchor_index\": doc_anchor_pairs}\n",
    "rdd_dict = {\"full_lemmatized_body_index\": doc_text_pairs}\n",
    "for rdd_name, rdd_pairs in rdd_dict.items():\n",
    "    # for anchors - need to handle duplicated anchors found on different pages pointing to the same page with similar or different anchor text\n",
    "    if (rdd_name == \"full_lemmatized_anchor_index\"):\n",
    "        print(\"running anchorIndex\")\n",
    "        doc_tok = rdd_pairs.flatMap(lambda x: x[0]).flatMap(lambda x: get_tokens(x[1], x[0])).reduceByKey(lambda x,y: x+y).mapValues(set)\n",
    "    else:\n",
    "        doc_tok = rdd_pairs.flatMap(lambda x: get_tokens(x[0], x[1]))\n",
    "    \n",
    "    # calculate document length for later tf normalization\n",
    "    doc_length = doc_tok.flatMap(lambda x: get_doc_len(x[0], x[1]))\n",
    "    # calculate term frequency by document\n",
    "    word_counts = doc_tok.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "    if (rdd_name == \"full_lemmatized_body_index\"):\n",
    "        print(\"running bodyIndex with uncommon_words filter\\n\")\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts).filter(lambda x: len(x[1]) >= 50)\n",
    "    else:\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    # Calculate term document frequency\n",
    "    w2df = calculate_df(postings)\n",
    "    # Calculate norm of each document - get tf from posting, df from w2df, doc length and calculate tfidf^2 per doc_id, term and sum by doc_id\n",
    "    doc2norm = postings.flatMapValues(lambda x: x).leftOuterJoin(w2df) \\\n",
    "        .map(lambda x: (x[1][0][0], (x[0], x[1][0][1], x[1][1]))) \\\n",
    "        .leftOuterJoin(doc_length).map(lambda x: (x[0], (x[1][0][1], x[1][0][2], x[1][1]))) \\\n",
    "        .mapValues(lambda x: pow(get_tfidf(x[0], x[1], x[2]), 2)) \\\n",
    "        .reduceByKey(lambda x,y: x+y).mapValues(lambda x: round(x, 6))\n",
    "\n",
    "    # save doc_id: (doc_norm, doc_len) RDD for cosin similarity calculations\n",
    "    doc_data = doc2norm.join(doc_length)\n",
    "\n",
    "    # write posting to bin files\n",
    "    _ = partition_postings_and_write(postings, bucket_name, rdd_name).collect()\n",
    "    print(f\"PostingLocs created for {rdd_name}\\n\")\n",
    "    \n",
    "    super_posting_locs = defaultdict(list)\n",
    "    for blob in client.list_blobs(bucket_name, prefix=rdd_name):\n",
    "        if not blob.name.endswith(\"pickle\"):\n",
    "            continue\n",
    "        with blob.open(\"rb\") as f:\n",
    "            posting_locs = pickle.load(f)\n",
    "            for k, v in posting_locs.items():\n",
    "                super_posting_locs[k].extend(v)\n",
    "                \n",
    "    # Create inverted index instance\n",
    "    inverted = InvertedIndex()\n",
    "    # Adding the posting locations dictionary to the inverted index\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "    # Add the token - df dictionary to the inverted index\n",
    "    inverted.df.update(w2df.collectAsMap())\n",
    "    print(f\"document frequancy created for {rdd_name}\\n\")\n",
    "    # Count number of docs\n",
    "    inverted._N = n_pages\n",
    "    # Get each document length and norm\n",
    "    inverted.doc_data.update(doc_data.collectAsMap())\n",
    "    print(f\"doc data created for {rdd_name}\\n\")\n",
    "    # save titles to return results\n",
    "    inverted.doc2title = id2title\n",
    "    # write the global stats out\n",
    "    inverted.write_index('.', rdd_name)\n",
    "    print(f\"{rdd_name} written\\n\")\n",
    "    \n",
    "    # upload to gs\n",
    "    index_src = f\"{rdd_name}.pkl\"\n",
    "    index_dst = f'gs://{bucket_name}/{rdd_name}/{index_src}'\n",
    "    !gsutil cp $index_src $index_dst\n",
    "    print(f\"{rdd_name} uploaded to bucket\\n\")\n",
    "    \n",
    "    !gsutil ls -lh $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983faa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
