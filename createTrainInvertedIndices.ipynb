{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3c8f4",
   "metadata": {
    "id": "58b3c8f4",
    "outputId": "006853c7-29be-4c59-bb5d-95a87d235bca"
   },
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647c4bf",
   "metadata": {
    "id": "7647c4bf",
    "outputId": "5c56dae1-6810-4f87-cccb-36552d9d162c"
   },
   "outputs": [],
   "source": [
    "# !pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508edd60",
   "metadata": {
    "id": "508edd60",
    "outputId": "686014fb-51da-4c0a-873a-34ce16edbd43"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import math\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b42bad7",
   "metadata": {
    "id": "8b42bad7"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as pyfunc\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d623c9b3",
   "metadata": {
    "id": "d623c9b3"
   },
   "outputs": [],
   "source": [
    "bucket_name = '201640042_project' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name.endswith('parquet'):\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f937eabc",
   "metadata": {
    "id": "f937eabc",
    "outputId": "6e46c386-bd46-42d6-b706-134565d9dce3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "doc_anchor_pairs = parquetFile.select(\"anchor_text\", \"id\").rdd\n",
    "n_pages = parquetFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b0c928",
   "metadata": {
    "id": "29b0c928"
   },
   "outputs": [],
   "source": [
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea31e28",
   "metadata": {
    "id": "3ea31e28"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a4035b",
   "metadata": {
    "id": "02a4035b"
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "# # Opening JSON file\n",
    "# import json\n",
    "# with open('/home/dataproc/queries_train.json') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "# train_tokens = [token.group() for token in RE_WORD.finditer(' '.join(data.keys()).lower())]\n",
    "# train_filtered_vocab = set([tok for tok in train_tokens if (tok not in all_stopwords)])\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def get_tokens(text, doc_id):\n",
    "    \"\"\"Support function - performs token filtering per doc \n",
    "      Parameters:\n",
    "    -----------\n",
    "    text: str\n",
    "      Text of one document\n",
    "    id: int\n",
    "      Document id\n",
    "    Returns:\n",
    "    --------\n",
    "    List of tuples\n",
    "      A list of (doc_id, filtered_token_list) pairs \n",
    "    \"\"\"\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token != '']\n",
    "    filtered_tokens = [tok for tok in tokens if (tok not in all_stopwords)]\n",
    "    return [(doc_id, filtered_tokens)]\n",
    "\n",
    "\n",
    "def word_count(doc_id, tokens):\n",
    "    \"\"\" Count the frequency of each word in `text` (tf) that is not included in \n",
    "    `all_stopwords` and return entries that will go into our posting lists. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    id: int\n",
    "      Document id\n",
    "    tokens: str\n",
    "      list of tokens from document\n",
    "    Returns:\n",
    "    --------\n",
    "    List of tuples\n",
    "      A list of (token, (doc_id, tf)) pairs \n",
    "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "    \"\"\"\n",
    "    doc_word_count = Counter(tokens)\n",
    "    return [(tok, (doc_id, tf)) for tok, tf in doc_word_count.items()]\n",
    "\n",
    "def get_doc_len(doc_id, tokens):\n",
    "    \"\"\" Count document filtered length for storage in index as well as document vector length for RDD calculations\n",
    "  Parameters:\n",
    "  -----------\n",
    "    id: int\n",
    "      Document id\n",
    "    tokens: str\n",
    "      list of tokens from document\n",
    "    Returns:\n",
    "  --------\n",
    "    List of tuples\n",
    "      A list of (doc_id, doc_length) pairs\n",
    "  \"\"\"\n",
    "    doc_word_count = Counter(tokens)\n",
    "    doc_len = sum(doc_word_count.values())\n",
    "    return [(doc_id, doc_len)]\n",
    "    \n",
    "\n",
    "def get_tfidf(tf, df, doc_len):\n",
    "    if doc_len == 0:\n",
    "        return 0.0\n",
    "    tf_idf = tf/doc_len * math.log(n_pages / (df + 1), 2)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fac7e82",
   "metadata": {
    "id": "5fac7e82",
    "outputId": "0574c9ff-8c0e-4f01-8d97-f9fd052a8e92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get title dict for inverted index to quickly prepare results\n",
    "id2title = doc_title_pairs.map(lambda x: (x[1], x[0])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab08e3c",
   "metadata": {
    "id": "6ab08e3c",
    "outputId": "6484eab2-c937-4af3-db9e-7727fb595423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running anchorIndex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostingLocs created for full_anchor_index\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document frequancy created for full_anchor_index\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc data created for full_anchor_index\n",
      "\n",
      "full_anchor_index written\n",
      "\n",
      "Copying file://full_anchor_index.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][364.6 MiB/364.6 MiB]                                                \n",
      "Operation completed over 1 objects/364.6 MiB.                                    \n",
      "full_anchor_index uploaded to bucket\n",
      "\n",
      "364.56 MiB  2023-01-10T13:55:13Z  gs://201640042_project/full_anchor_index/full_anchor_index.pkl\n",
      "TOTAL: 1 objects, 382263773 bytes (364.56 MiB)\n"
     ]
    }
   ],
   "source": [
    "# rdd_dict = {\"full_body_index\": doc_text_pairs, \"full_title_index\": doc_title_pairs, \"full_anchor_index\": doc_anchor_pairs}\n",
    "rdd_dict = {\"full_anchor_index\": doc_anchor_pairs}\n",
    "for rdd_name, rdd_pairs in rdd_dict.items():\n",
    "    # for anchors - need to handle duplicated anchors found on different pages pointing to the same page with similar or different anchor text\n",
    "    if (rdd_name == \"full_anchor_index\"):\n",
    "        print(\"running anchorIndex\")\n",
    "        doc_tok = rdd_pairs.flatMap(lambda x: x[0]).flatMap(lambda x: get_tokens(x[1], x[0])).reduceByKey(lambda x,y: x+y).mapValues(set)\n",
    "    else:\n",
    "        doc_tok = rdd_pairs.flatMap(lambda x: get_tokens(x[0], x[1]))\n",
    "    \n",
    "    # calculate document length for later tf normalization\n",
    "    doc_length = doc_tok.flatMap(lambda x: get_doc_len(x[0], x[1]))\n",
    "    # calculate term frequency by document\n",
    "    word_counts = doc_tok.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "    if (rdd_name == \"full_body_index\"):\n",
    "        print(\"running bodyIndex with uncommon_words filter\\n\")\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts).filter(lambda x: len(x[1]) >= 50)\n",
    "    else:\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    # Calculate term document frequency\n",
    "    w2df = calculate_df(postings)\n",
    "    # Calculate norm of each document - get tf from posting, df from w2df, doc length and calculate tfidf^2 per doc_id, term and sum by doc_id\n",
    "    doc2norm = postings.flatMapValues(lambda x: x).leftOuterJoin(w2df) \\\n",
    "        .map(lambda x: (x[1][0][0], (x[0], x[1][0][1], x[1][1]))) \\\n",
    "        .leftOuterJoin(doc_length).map(lambda x: (x[0], (x[1][0][1], x[1][0][2], x[1][1]))) \\\n",
    "        .mapValues(lambda x: math.pow(get_tfidf(x[0], x[1], x[2]), 2)) \\\n",
    "        .reduceByKey(lambda x,y: x+y).mapValues(lambda x: round(x, 6))\n",
    "\n",
    "    # save doc_id: (doc_norm, doc_len) RDD for cosin similarity calculations\n",
    "    doc_data = doc2norm.join(doc_length)\n",
    "\n",
    "    # write posting to bin files\n",
    "    _ = partition_postings_and_write(postings, bucket_name, rdd_name).collect()\n",
    "    print(f\"PostingLocs created for {rdd_name}\\n\")\n",
    "    \n",
    "    super_posting_locs = defaultdict(list)\n",
    "    for blob in client.list_blobs(bucket_name, prefix=rdd_name):\n",
    "        if not blob.name.endswith(\"pickle\"):\n",
    "            continue\n",
    "        with blob.open(\"rb\") as f:\n",
    "            posting_locs = pickle.load(f)\n",
    "            for k, v in posting_locs.items():\n",
    "                super_posting_locs[k].extend(v)\n",
    "                \n",
    "    # Create inverted index instance\n",
    "    inverted = InvertedIndex()\n",
    "    # Adding the posting locations dictionary to the inverted index\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "    # Add the token - df dictionary to the inverted index\n",
    "    inverted.df.update(w2df.collectAsMap())\n",
    "    print(f\"document frequancy created for {rdd_name}\\n\")\n",
    "    # Count number of docs\n",
    "    inverted._N = n_pages\n",
    "    # Get each document length and norm\n",
    "    inverted.doc_data.update(doc_data.collectAsMap())\n",
    "    print(f\"doc data created for {rdd_name}\\n\")\n",
    "    # save titles to return results\n",
    "    inverted.doc2title = id2title\n",
    "    # write the global stats out\n",
    "    inverted.write_index('.', rdd_name)\n",
    "    print(f\"{rdd_name} written\\n\")\n",
    "    \n",
    "    # upload to gs\n",
    "    index_src = f\"{rdd_name}.pkl\"\n",
    "    index_dst = f'gs://{bucket_name}/{rdd_name}/{index_src}'\n",
    "    !gsutil cp $index_src $index_dst\n",
    "    print(f\"{rdd_name} uploaded to bucket\\n\")\n",
    "    \n",
    "    !gsutil ls -lh $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbfb6c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with full_anchor_index\n"
     ]
    }
   ],
   "source": [
    "print(f\"done with {rdd_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82d27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(pages):\n",
    "    ''' Compute the directed graph generated by wiki links.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    pages: RDD\n",
    "      An RDD where each row consists of one wikipedia articles with 'id' and \n",
    "      'anchor_text'.\n",
    "  Returns:\n",
    "  --------\n",
    "    edges: RDD\n",
    "      An RDD where each row represents an edge in the directed graph created by\n",
    "      the wikipedia links. The first entry should the source page id and the \n",
    "      second entry is the destination page id. No duplicates should be present. \n",
    "    vertices: RDD\n",
    "      An RDD where each row represents a vetrix (node) in the directed graph \n",
    "      created by the wikipedia links. No duplicates should be present. \n",
    "  '''\n",
    "    edges = pages.map(lambda page: [(page[0], link_id.id) for link_id in page[1]]).flatMap(lambda ls: ls).distinct()\n",
    "    vertices = edges.map(lambda edge: [edge[0],edge[1]]).flatMap(lambda ls: ls).distinct().map(lambda x: (x, ))\n",
    "    return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199c0886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GraphFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m edgesDF \u001b[38;5;241m=\u001b[39m edges\u001b[38;5;241m.\u001b[39mtoDF([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdst\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m124\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m verticesDF \u001b[38;5;241m=\u001b[39m vertices\u001b[38;5;241m.\u001b[39mtoDF([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m124\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mGraphFrame\u001b[49m(verticesDF, edgesDF)\n\u001b[1;32m      8\u001b[0m pr_results \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mpageRank(resetProbability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      9\u001b[0m pr \u001b[38;5;241m=\u001b[39m pr_results\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GraphFrame' is not defined"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "# construct the graph \n",
    "edges, vertices = generate_graph(parquetFile.select(\"id\", \"anchor_text\").rdd)\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a4909",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pr_results \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mpageRank(resetProbability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      3\u001b[0m pr \u001b[38;5;241m=\u001b[39m pr_results\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pr \u001b[38;5;241m=\u001b[39m pr\u001b[38;5;241m.\u001b[39msort(\u001b[43mcol\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n\u001b[1;32m      5\u001b[0m pr_time \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m      6\u001b[0m pr\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "494b757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 210:===================================================> (195 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|     id|          pagerank|\n",
      "+-------+------------------+\n",
      "|3434750| 9913.728782160777|\n",
      "|  10568| 5385.349263642033|\n",
      "|  32927| 5282.081575765273|\n",
      "|  30680|  5128.23370960412|\n",
      "|5843419| 4957.567686263868|\n",
      "|  68253|4769.2782653551585|\n",
      "|  31717|  4486.35018054831|\n",
      "|  11867| 4146.414650912772|\n",
      "|  14533|3996.4664408854983|\n",
      "| 645042|3531.6270898037437|\n",
      "|  17867| 3246.098390604142|\n",
      "|5042916|  2991.94573916618|\n",
      "|4689264|2982.3248830417483|\n",
      "|  14532|2934.7468292031704|\n",
      "|  25391| 2903.546223513398|\n",
      "|   5405| 2891.416329154636|\n",
      "|4764461|2834.3669873326617|\n",
      "|  15573| 2783.865118158838|\n",
      "|   9316|2782.0396464137684|\n",
      "|8569916|2775.2861918400167|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pr = pr.sort(pyfunc.col('pagerank').desc())\n",
    "pr_time = time() - t_start\n",
    "pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9aaf4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pagerank = pr.rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf607ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(f'{bucket_name}')\n",
    "blob = bucket.blob(\"pagerank_org.pkl\")\n",
    "with blob.open(\"wb\") as f:\n",
    "    pickle.dump(pagerank, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0983ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
