{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project summary notebook for report"
      ],
      "metadata": {
        "id": "sAuWSaN7B4ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXul29X8_C86",
        "outputId": "3a75c04a-4789-45e4-8369-15719d6f1899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.8/dist-packages (2.0.1)\n",
            "openjdk-8-jdk-headless is already the newest version (8u352-ga-1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "--2023-01-05 22:03:40--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 18.165.83.25, 18.165.83.55, 18.165.83.74, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|18.165.83.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ not modified on server. Omitting download.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import json\n",
        " \n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "!pip install -q pyspark\n",
        "!pip install findspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install -q graphframes\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
        "spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
        "!wget -N -P $spark_jars $graphframes_jar\n",
        "from inverted_index_colab import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build small inverted index from only trainQuery words"
      ],
      "metadata": {
        "id": "Mhx1JNBDB-aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "import signal\n",
        "\n",
        "def handler(signum, frame):\n",
        "  raise Exception(\"Authentication timeout!\")\n",
        "\n",
        "signal.signal(signal.SIGALRM, handler)\n",
        "\n",
        "try:\n",
        "   auth.authenticate_user()\n",
        "except: \n",
        "   pass\n",
        "# Copy one wikidumps files \n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import auth\n",
        "## RENAME the project_id to yours project id from the project you created in GCP \n",
        "project_id = 'inforetassignment3'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "data_bucket_name = '201640042_project'"
      ],
      "metadata": {
        "id": "bj2JjxxMHu1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4488dae-052f-4e21-ed14-e5ccdb221b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "        pass  \n",
        "# can also just download all the bucket !gsutil -u {project_id} cp gs://{data_bucket_name}\n",
        "except:\n",
        "      !mkdir wikidumps\n",
        "      !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\"\n",
        "\n",
        "# for i in range(2, 27):\n",
        "#   !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream{i}_preprocessed.parquet \"wikidumps/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-iAl5h9KNsM",
        "outputId": "faec9ae9-e3f7-4f90-9426-de7c2173abb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://201640042_project/multistream1_preprocessed.parquet...\n",
            "- [1 files][316.7 MiB/316.7 MiB]                                                \n",
            "Operation completed over 1 objects/316.7 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init() \n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from graphframes import *\n",
        "\n",
        "\n",
        "\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "Ym_LN19EPhQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n",
        "except:\n",
        "      path = \"wikidumps/*\"\n",
        "parquetFile = spark.read.parquet(path)\n",
        "# parquetFile.show()"
      ],
      "metadata": {
        "id": "yW2JFx1zPXxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take the 'text' and 'id' or the first 1000 rows and create an RDD from it\n",
        "doc_text_pairs = parquetFile.limit(1000).select(\"text\", \"id\").rdd\n",
        "# doc_anchor_pairs = parquetFile.limit(1000).select(\"anchor_text\", \"id\").rdd\n",
        "# doc_title_pairs = parquetFile.limit(1000).select(\"title\", \"id\").rdd"
      ],
      "metadata": {
        "id": "SI0Cb1-7Pxl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "# Opening JSON file\n",
        "with open('queries_train.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "train_tokens = [token.group() for token in RE_WORD.finditer(' '.join(data.keys()).lower())]\n",
        "train_filtered_vocab = set([tok for tok in train_tokens if (tok not in all_stopwords)])\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  ''' Count the frequency of each word in `text` (tf) that is not included in \n",
        "  `all_stopwords` and return entries that will go into our posting lists. \n",
        "  Parameters:\n",
        "  -----------\n",
        "    text: str\n",
        "      Text of one document\n",
        "    id: int\n",
        "      Document id\n",
        "  Returns:\n",
        "  --------\n",
        "    List of tuples\n",
        "      A list of (token, (doc_id, tf)) pairs \n",
        "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
        "  '''\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  filtered_tokens = [tok for tok in tokens if ((tok not in all_stopwords) and (tok in train_filtered_vocab))]\n",
        "  doc_word_count = Counter(filtered_tokens)\n",
        "  return [(tok, (id, tf)) for tok, tf in doc_word_count.items()]"
      ],
      "metadata": {
        "id": "ikKG9dpoP-O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = doc_text_pairs.flatMap(lambda x: get_doc_len(x[0], x[1]))\n",
        "doc_length = res.map(lambda x: (x[0], x[1]))\n",
        "doc_vec_len = res.map(lambda x: (x[0], x[2]))\n",
        "doc2len = doc_length.collectAsMap()\n",
        "doc2vec_len = doc_vec_len.collectAsMap()"
      ],
      "metadata": {
        "id": "CM76pjc5nkt2"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "posting_locs_list = partition_postings_and_write(postings_filtered).collect()"
      ],
      "metadata": {
        "id": "x3w-xuHiMokP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_posting_locs = defaultdict(list)\n",
        "for posting_loc in posting_locs_list:\n",
        "  for k, v in posting_loc.items():\n",
        "    super_posting_locs[k].extend(v)"
      ],
      "metadata": {
        "id": "X3pPImQQRzNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create inverted index instance\n",
        "inverted = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted.posting_locs = super_posting_locs\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted.df = w2df_dict\n",
        "# Count number of docs\n",
        "inverted._N = parquetFile.count()\n",
        "# Get each document vector legnth\n",
        "inverted.doc2len = doc2len\n",
        "inverted.doc2vec_len = doc2vec_len\n",
        "# write the global stats out\n",
        "inverted.write_index('.', 'train_body_index')"
      ],
      "metadata": {
        "id": "aom_QvUZU59Q"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inverted = InvertedIndex().read_index('train_body_index')\n",
        "# with open('queries_train.json') as json_file:\n",
        "#     data = json.load(json_file)\n",
        "# data.keys()"
      ],
      "metadata": {
        "id": "6O-y5kZRVCGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fad98f4-b1e1-44c8-9db5-6415186b0275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['best marvel movie', 'How do kids come to world?', 'Information retrieval', 'LinkedIn', 'How to make coffee?', 'Ritalin', 'How to make wine at home?', 'Most expensive city in the world', 'India', 'how to make money fast?', 'Netflix', 'Apple computer', 'The Simpsons', 'World cup', 'How to lose weight?', 'Java', 'Air Jordan', 'how to deal with depression?', 'How do you make gold', 'Marijuana', 'How to make hummus', 'Winter', 'Rick and Morty', 'Natural Language processing', 'World Cup 2022', 'Dolly the sheep', 'Ciggarets', 'What is the best place to live in?', 'Elon musk', 'How do you breed flowers?'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "super_posting_locs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvHWelVh5j7v",
        "outputId": "916d6195-2714-4bb1-c311-b2988dabb8c7"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'world': [('56_000.bin', 0)],\n",
              "             'best': [('87_000.bin', 0)],\n",
              "             'live': [('12_000.bin', 0)],\n",
              "             'place': [('66_000.bin', 0)],\n",
              "             'make': [('51_000.bin', 0)],\n",
              "             'air': [('72_000.bin', 0)],\n",
              "             'information': [('11_000.bin', 0)],\n",
              "             'processing': [('5_000.bin', 0)],\n",
              "             'apple': [('5_000.bin', 486)],\n",
              "             'language': [('21_000.bin', 0)],\n",
              "             'natural': [('24_000.bin', 0)],\n",
              "             'depression': [('115_000.bin', 0)],\n",
              "             'home': [('63_000.bin', 0)],\n",
              "             'weight': [('93_000.bin', 0)],\n",
              "             'lose': [('89_000.bin', 0)],\n",
              "             'deal': [('27_000.bin', 0)],\n",
              "             'winter': [('17_000.bin', 0)],\n",
              "             'come': [('8_000.bin', 0)],\n",
              "             'cup': [('28_000.bin', 0)],\n",
              "             'city': [('104_000.bin', 0)],\n",
              "             'fast': [('104_000.bin', 2448)],\n",
              "             'money': [('73_000.bin', 0)],\n",
              "             'computer': [('83_000.bin', 0)],\n",
              "             'gold': [('6_000.bin', 0)],\n",
              "             'india': [('6_000.bin', 1158)],\n",
              "             'wine': [('99_000.bin', 0)],\n",
              "             'movie': [('14_000.bin', 0)],\n",
              "             'jordan': [('120_000.bin', 0)],\n",
              "             'expensive': [('7_000.bin', 0)]})"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwT3Mb6q6dEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}